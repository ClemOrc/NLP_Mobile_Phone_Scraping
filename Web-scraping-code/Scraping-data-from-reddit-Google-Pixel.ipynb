{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data using reddit-api (only posts related to the attributes to begin with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: Ok-Resist-1662\n",
      "Scraping subreddit: smartphones\n",
      "Scraping subreddit: Android\n",
      "Scraping subreddit: apple\n",
      "Scraping subreddit: Samsung\n",
      "Scraping subreddit: GooglePixel\n",
      "Scraping subreddit: motorola\n",
      "Scraping subreddit: OnePlus\n",
      "Scraping subreddit: Xiaomi\n",
      "Data saved to reddit_posts_last_6_months.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Load .env file for credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API authentication\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    username=os.getenv(\"USERNAMES\"),  # Ensure correct key name in your .env file\n",
    "    password=os.getenv(\"PASSWORD\")\n",
    ")\n",
    "\n",
    "# Verify authentication\n",
    "try:\n",
    "    print(f\"Authenticated as: {reddit.user.me()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Scraping data\n",
    "all_posts = []\n",
    "for subreddit_name in subreddits:\n",
    "    print(f\"Scraping subreddit: {subreddit_name}\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    for post in subreddit.new(limit=1000):  # Fetch up to 1000 posts per subreddit\n",
    "        post_date = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)\n",
    "        if post_date >= start_date:\n",
    "            all_posts.append({\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"title\": post.title,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"created_utc\": post_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"score\": post.score,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"url\": post.url\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"reddit_posts_last_6_months.csv\"\n",
    "# df.to_csv(csv_filename, index=False)\n",
    "print(f\"Data saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7448, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: Ok-Resist-1662\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: Ok-Resist-1662\n",
      "Scraping subreddit: GooglePixel\n",
      "Scraping subreddit: Android\n",
      "Scraping subreddit: Smartphones\n",
      "Scraping subreddit: TechNews\n",
      "Scraping subreddit: Gadgets\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Load .env file for credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API authentication\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    username=os.getenv(\"USERNAMES\"),  # Ensure correct key name in your .env file\n",
    "    password=os.getenv(\"PASSWORD\")\n",
    ")\n",
    "\n",
    "# Verify authentication\n",
    "try:\n",
    "    print(f\"Authenticated as: {reddit.user.me()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Define subreddits to scrape\n",
    "subreddits = [\"GooglePixel\", \"Android\", \"Smartphones\", \"TechNews\", \"Gadgets\"]\n",
    "\n",
    "# Define the time range (e.g., last 6 months)\n",
    "start_date = datetime.now(timezone.utc) - timedelta(days=1800)\n",
    "\n",
    "# Scraping data\n",
    "all_posts = []\n",
    "for subreddit_name in subreddits:\n",
    "    print(f\"Scraping subreddit: {subreddit_name}\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    for post in subreddit.new(limit=1000):  # Fetch up to 1000 posts per subreddit\n",
    "        post_date = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)\n",
    "        if post_date >= start_date:\n",
    "            # Check if the post title or text contains \"Pixel 9 Pro\"\n",
    "            if \"Pixel 9\" in post.title or \"Pixel 9 Pro\" in post.title or \"Pixel 9 Pro\" in post.selftext or \"Pixel 9\" in post.selftext:\n",
    "                all_posts.append({\n",
    "                    \"subreddit\": subreddit_name,\n",
    "                    \"title\": post.title,\n",
    "                    \"selftext\": post.selftext,\n",
    "                    \"created_utc\": post_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"score\": post.score,\n",
    "                    \"num_comments\": post.num_comments,\n",
    "                    \"url\": post.url\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# # Save to CSV\n",
    "# csv_filename = \"reddit_posts_pixel_9_pro_last_6_months.csv\"\n",
    "# df.to_csv(csv_filename, index=False)\n",
    "# print(f\"Data saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: Ok-Resist-1662\n",
      "Scraping subreddit: GooglePixel\n",
      "Scraping subreddit: Android\n",
      "Scraping subreddit: Smartphones\n",
      "Scraping subreddit: TechNews\n",
      "Scraping subreddit: Gadgets\n",
      "Data saved to reddit_posts_pixel_9_pro.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Load .env file for credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API authentication\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    username=os.getenv(\"USERNAMES\"),  # Ensure correct key name in your .env file\n",
    "    password=os.getenv(\"PASSWORD\")\n",
    ")\n",
    "\n",
    "# Verify authentication\n",
    "try:\n",
    "    print(f\"Authenticated as: {reddit.user.me()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define subreddits to scrape\n",
    "subreddits = [\"GooglePixel\", \"Android\", \"Smartphones\", \"TechNews\", \"Gadgets\"]\n",
    "\n",
    "# Define the time range (e.g., last 6 months)\n",
    "start_date = datetime.now(timezone.utc) - timedelta(days=180)\n",
    "\n",
    "# Scraping data\n",
    "all_posts = []\n",
    "for subreddit_name in subreddits:\n",
    "    print(f\"Scraping subreddit: {subreddit_name}\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    for post in subreddit.new(limit=1000):  # Fetch up to 1000 posts per subreddit\n",
    "        post_date = datetime.fromtimestamp(post.created_utc, tz=timezone.utc)\n",
    "        if post_date >= start_date:\n",
    "            # Check if the post title or text contains \"Pixel 9 Pro\"\n",
    "            if \"Pixel 9\" in post.title or \"Pixel 9 Pro\" in post.title or \"Pixel 9 Pro\" in post.selftext or \"Pixel 9\" in post.selftext:\n",
    "                all_posts.append({\n",
    "                    \"post_id\": post.id,  # Store post ID for comment retrieval\n",
    "                    \"subreddit\": subreddit_name,\n",
    "                    \"title\": post.title,\n",
    "                    \"selftext\": post.selftext,\n",
    "                    \"created_utc\": post_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"score\": post.score,\n",
    "                    \"num_comments\": post.num_comments,\n",
    "                    \"url\": post.url\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"reddit_posts_pixel_9_pro.csv\"\n",
    "# df.to_csv(csv_filename, index=False)\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to reddit_posts_pixel_9_pro.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "csv_filename = \"reddit_posts_pixel_9_pro.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Data saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1iksqit</td>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>Continued conversation with Google Assistant n...</td>\n",
       "      <td>Hi, I'm using Pixel 9 Pro and my problem is th...</td>\n",
       "      <td>2025-02-08 17:36:44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/GooglePixel/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ikrwa8</td>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>Pixel 9 Pro horizontal green line after screen...</td>\n",
       "      <td>A green line shows up close to the bottom of t...</td>\n",
       "      <td>2025-02-08 17:01:37</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/GooglePixel/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ikr1yi</td>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>Help with Google Photos</td>\n",
       "      <td>Helping my 75 years old father-in-law as he wa...</td>\n",
       "      <td>2025-02-08 16:25:56</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/GooglePixel/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ikpwsz</td>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>My thoughts on the Pixel 9 Pro XL</td>\n",
       "      <td>Warning: negative experience ahead. Fanboys be...</td>\n",
       "      <td>2025-02-08 15:36:00</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>https://www.reddit.com/r/GooglePixel/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ikov5c</td>\n",
       "      <td>GooglePixel</td>\n",
       "      <td>Hi, I upgraded my Galaxy S21 to a Google Pixel...</td>\n",
       "      <td>On the Samsung phone I could download an app c...</td>\n",
       "      <td>2025-02-08 14:49:25</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>https://www.reddit.com/r/GooglePixel/comments/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id    subreddit                                              title  \\\n",
       "0  1iksqit  GooglePixel  Continued conversation with Google Assistant n...   \n",
       "1  1ikrwa8  GooglePixel  Pixel 9 Pro horizontal green line after screen...   \n",
       "2  1ikr1yi  GooglePixel                            Help with Google Photos   \n",
       "3  1ikpwsz  GooglePixel                  My thoughts on the Pixel 9 Pro XL   \n",
       "4  1ikov5c  GooglePixel  Hi, I upgraded my Galaxy S21 to a Google Pixel...   \n",
       "\n",
       "                                            selftext          created_utc  \\\n",
       "0  Hi, I'm using Pixel 9 Pro and my problem is th...  2025-02-08 17:36:44   \n",
       "1  A green line shows up close to the bottom of t...  2025-02-08 17:01:37   \n",
       "2  Helping my 75 years old father-in-law as he wa...  2025-02-08 16:25:56   \n",
       "3  Warning: negative experience ahead. Fanboys be...  2025-02-08 15:36:00   \n",
       "4  On the Samsung phone I could download an app c...  2025-02-08 14:49:25   \n",
       "\n",
       "   score  num_comments                                                url  \n",
       "0      1             0  https://www.reddit.com/r/GooglePixel/comments/...  \n",
       "1      3             0  https://www.reddit.com/r/GooglePixel/comments/...  \n",
       "2      2             4  https://www.reddit.com/r/GooglePixel/comments/...  \n",
       "3      0            39  https://www.reddit.com/r/GooglePixel/comments/...  \n",
       "4     45            11  https://www.reddit.com/r/GooglePixel/comments/...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded data saved to reddit_posts_expanded_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file for credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API authentication\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    username=os.getenv(\"USERNAMES\"),\n",
    "    password=os.getenv(\"PASSWORD\")\n",
    ")\n",
    "\n",
    "# Load the saved posts CSV\n",
    "csv_filename = \"reddit_posts_pixel_9_pro.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Ensure 'selftext' is treated as a string and replace NaN values\n",
    "df[\"selftext\"] = df[\"selftext\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Fetch comments and store each as a separate row\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    post_id = row[\"post_id\"]\n",
    "    try:\n",
    "        submission = reddit.submission(id=post_id)\n",
    "        submission.comments.replace_more(limit=0)  # Load all top-level comments\n",
    "\n",
    "        # If there are no comments, still keep the post with only selftext\n",
    "        if len(submission.comments) == 0:\n",
    "            expanded_rows.append({\n",
    "                \"post_id\": row[\"post_id\"],\n",
    "                \"subreddit\": row[\"subreddit\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"text\": row[\"selftext\"],  # Only post text (no comments)\n",
    "                \"created_utc\": row[\"created_utc\"],\n",
    "                \"score\": row[\"score\"],\n",
    "                \"num_comments\": row[\"num_comments\"],\n",
    "                \"url\": row[\"url\"],\n",
    "                \"comment_id\": None,  # No comment\n",
    "                \"comment_text\": None  # No comment\n",
    "            })\n",
    "        else:\n",
    "            for comment in submission.comments.list():\n",
    "                expanded_rows.append({\n",
    "                    \"post_id\": row[\"post_id\"],\n",
    "                    \"subreddit\": row[\"subreddit\"],\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"text\": row[\"selftext\"],  # Keep the original post text\n",
    "                    \"created_utc\": row[\"created_utc\"],\n",
    "                    \"score\": row[\"score\"],\n",
    "                    \"num_comments\": row[\"num_comments\"],\n",
    "                    \"url\": row[\"url\"],\n",
    "                    \"comment_id\": comment.id,  # Store comment ID\n",
    "                    \"comment_text\": comment.body  # Store the comment\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch comments for post {post_id}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_expanded = pd.DataFrame(expanded_rows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded data saved to reddit_posts_expanded_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Save updated data to CSV\n",
    "expanded_csv_filename = \"reddit_posts_expanded_comments.csv\"\n",
    "df_expanded.to_csv(expanded_csv_filename, index=False)\n",
    "print(f\"Expanded data saved to {expanded_csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hp\\\\Documents\\\\GitHub\\\\LLM-Enhanced-Text-Classification\\\\Web-scraping-code'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
